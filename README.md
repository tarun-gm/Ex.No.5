
---

# **EXP 5: Comparative Study of Prompting Approaches Across Multiple Scenarios**

Date:05.09.2025
Register Number:212223060284

---

## **Aim**

To evaluate how ChatGPT responds when provided with **unstructured (naïve) prompts** compared to **structured (refined/basic) prompts**. The experiment focuses on measuring the **quality, accuracy, and depth** of outputs across different tasks.

---

## **AI Tool Utilized**

* ChatGPT (OpenAI)

---

## **Concept Explanation**

Two prompt categories are compared:

1. **Naïve Prompt** – Short, vague, or general instructions without proper context.
2. **Refined Prompt** – Well-structured, explicit, and detailed instructions with clarity.

By testing both prompt types across scenarios, the experiment highlights how **prompt engineering** directly affects AI-generated outputs.

---

## **Experimental Procedure**

1. Define different **test scenarios** (story writing, factual Q\&A, summarization, advice).
2. Design a **naïve prompt** and a **refined prompt** for each case.
3. Run both prompts in ChatGPT and **document responses**.
4. Compare results on **three criteria**:

   * Quality (relevance & fluency)
   * Accuracy (factual correctness)
   * Depth (level of detail & reasoning).
5. Draw insights on the importance of structured prompting.

---

## **Test Scenarios & Observations**

| **Scenario**          | **Naïve Prompt**            | **Response (Naïve)**                  | **Refined Prompt**                                                             | **Response (Refined)**                               | **Comparison**                               |
| --------------------- | --------------------------- | ------------------------------------- | ------------------------------------------------------------------------------ | ---------------------------------------------------- | -------------------------------------------- |
| Creative Story        | “Write a story.”            | Very short, lacks flow, generic plot. | “Write a 200-word moral story for children about honesty.”                     | Well-structured, age-appropriate, moral highlighted. | Refined prompt gave better quality & depth.  |
| Factual Question      | “Tell me about the moon.”   | Basic facts, short lines.             | “Explain the origin, surface features, and role of the moon in tides.”         | Detailed, accurate scientific explanation.           | Refined prompt improved accuracy & coverage. |
| Summarization         | “Summarize climate change.” | 2–3 vague lines.                      | “Summarize the causes, effects, and solutions of climate change in 150 words.” | Organized summary with causes, impacts, solutions.   | Refined prompt yielded structured depth.     |
| Advice/Recommendation | “Give me advice.”           | Very general tips.                    | “Suggest three daily health tips for office workers to reduce stress.”         | Practical, actionable, and specific advice.          | Refined prompt more useful and relevant.     |

---

## **Analysis**

* **Naïve prompts** often generate **surface-level answers** with limited structure.
* **Refined prompts** consistently improved **accuracy**, added **depth**, and tailored responses to the task.
* In creative tasks, refined prompts ensured **focus and direction**.
* However, for very simple tasks, naïve prompts sometimes produced acceptable answers.

---

## **Conclusion**

The experiment proves that **structured prompting significantly enhances AI response quality**. Clear instructions help ChatGPT provide outputs that are **more accurate, detailed, and context-specific**. Naïve prompts may work for simple queries, but refined prompts are essential for complex tasks.

---

## **Result**

Thus, the comparative analysis between naïve and refined prompting techniques was successfully executed, and the role of prompt clarity in improving AI outputs was demonstrated.

---
